I"!<p class="notice--info">All of this series is mainly based on the Machine Learning course given by Andrew Ng, which is hosted on <a href="https://www.coursera.org/">cousera.org</a>.</p>

<h2 id="recommender-system">Recommender System</h2>
<h3 id="predicting-movie-rating">Predicting Movie Rating</h3>

<p><img src="/media/15718172032236/15719679748913.jpg" alt="-w923" width="500px" /></p>

<p>Content Based Recommender system</p>

<p><img src="/media/15718172032236/15719919502453.jpg" alt="-w1438" width="500px" /></p>

<p>Note: For each movie i, we have some features, with $x^{(i)}_0=1$.</p>

<p><strong>Problem Formulation</strong></p>

<p><img src="/media/15718172032236/15719922485281.jpg" alt="-w543" width="500px" /></p>

<p><strong>Optimization Objective</strong>
<strong>To learn $\theta^{(j)} $ (parameters for user j):</strong></p>

\[\min _{\theta(j)} \frac{1}{2} \sum_{i: r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)^{2}+\frac{\lambda}{2} \sum_{k=1}^{n}\left(\theta_{k}^{(j)}\right)^{2}\]

<p><strong>To learn $\theta^{(j)}, \forall j$:</strong></p>

\[\min _{\theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}} \frac{1}{2} \sum_{j=1}^{n_{u}} \sum_{i: r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)^{2}+\frac{\lambda}{2} \sum_{j=1}^{n_{u}} \sum_{k=1}^{n}\left(\theta_{k}^{(j)}\right)^{2}\]

<p>My Thoughts:
After we get the $\theta \in R^{n+1}$ for each user, we can try to use PCA to divide the user into a limited number of representative groups: $Group_g,g=1,….,G$.
Since for these group, we can have more ratings available, we can get more information about this group, and derive better $\theta$ for each of these group.
After this, we find the approximation to $user_j$: which is a linear combination of $Group_g,g=1,….,G$.
The r(i,j) is then approximated by the linear combination of the rating for the groups.</p>

<h3 id="collaborative-filtering">Collaborative Filtering</h3>
<p>A algorithm which can find which feature to use.
After we already get the $\theta$ for the user, based on their ratings on a movie, we can estimate the features of the movie.</p>

<p><strong>Optimization Algorithm</strong></p>

\[\begin{array}{l}{\text { Given } \theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}, \text { to learn } x^{(i)}:} \\ {\quad \min _{x^{(i)}} \frac{1}{2} \sum_{j: r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)^{2}+\frac{\lambda}{2} \sum_{k=1}^{n}\left(x_{k}^{(i)}\right)^{2}}\end{array}\]

<p>Further, we can learn the features $x_{k}$ for all the movies.</p>

\[\begin{array}{l}{\text { Given } \theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}, \text { to learn } x^{(1)}, \ldots, x^{\left(n_{m}\right)}:} \\ {\qquad \begin{array}{l}{\min _{x^{(1)}, \ldots, x^{\left(n_{m}\right)}} \frac{1}{2} \sum_{i=1}^{n_{m}} \sum_{j: r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)^{2}+\frac{\lambda}{2} \sum_{i=1}^{n_{m}} \sum_{k=1}^{n}\left(x_{k}^{(i)}\right)^{2}}\end{array}} \\ \end{array}\]

<p><img src="/media/15718172032236/15719956163624.jpg" alt="-w495" width="500px" /></p>

<p><strong>Collaboration between users in the sense of better estimation of features:</strong> every user gives the system their preference, with which the system learn better feature x_i, and then further helping make the estimation of preference for each user.</p>

<h4 id="algorithm-for-collaborative-filtering">Algorithm for Collaborative Filtering</h4>

<p>$\text { Given } x^{(1)}, \ldots, x^{\left(n_{m}\right)}, \text { estimate } \theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}:$</p>

\[\min _{\theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}} \frac{1}{2} \sum_{j=1}^{n_{u}} \sum_{i: r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)^{2}+\frac{\lambda}{2} \sum_{j=1}^{n_{u}} \sum_{k=1}^{n}\left(\theta_{k}^{(j)}\right)^{2}\]

<p>${\text { Given } \theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}, \text { to learn } x^{(1)}, \ldots, x^{\left(n_{m}\right)}:}$</p>

\[\min _{x^{(1)}, \ldots, x^{\left(n_{m}\right)}} \frac{1}{2} \sum_{i=1}^{n_{m}} \sum_{j: r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)^{2}+\frac{\lambda}{2} \sum_{i=1}^{n_{m}} \sum_{k=1}^{n}\left(x_{k}^{(i)}\right)^{2}\]

<p>We combine this process:
$\text { Minimizing } x^{(1)}, \ldots, x^{\left(n_{m}\right)} \text { and } \theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)} \text { simultaneously: }$</p>

\[J\left(x^{(1)}, \ldots, x^{\left(n_{m}\right)}, \theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}\right)=\frac{1}{2} \sum_{(i, j): r(i, j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)^{2}+\frac{\lambda}{2} \sum_{i=1}^{n_{m}} \sum_{k=1}^{n}\left(x_{k}^{(i)}\right)^{2}+\frac{\lambda}{2} \sum_{j=1}^{n_{u}} \sum_{k=1}^{n}\left(\theta_{k}^{(j)}\right)^{2}\]

<p>The optimization problem:</p>

\[\min_{x,\theta} J(x,\theta)\]

<p>where:
$x=x^{(1)}, \ldots, x^{\left(n_{m}\right)}$
$\theta=\theta^{(1)}, \ldots, \theta^{\left(n_{u}\right)}$</p>

<p>Note: when we combine the two learning process, we can get rid of $x^{(i)}_0=1$. This is because the features can be learned during the process. If the algorithm really thinks a constant feature is needed, it will make one by itself during the iteration.</p>

<p>In conclusion:</p>

<p><img src="/media/15718172032236/15719970299401.jpg" alt="-w619" width="500px" /></p>

<p>Note:</p>
<ul>
  <li>here $x,\theta \in R^n$ without the constant term.</li>
  <li>The final predicted star rating is $\theta^T x$</li>
  <li>Initialization of the x and $\theta$ is for the purpose of breaking the symmetry and ensuring the learned $x_i$ and $\theta_j$ are different.</li>
</ul>

<h4 id="implementationvectorization-of-collaborative-filteringlow-rank-matrix-factorization">Implementation/vectorization of Collaborative Filtering–Low Rank matrix Factorization</h4>
<p>For the collaborative filtering, we basically deal with the problem of finding parameters given the result.</p>

<p><img src="/media/15718172032236/15720756142253.jpg" alt="-w701" width="500px" /></p>

<p>In other words:</p>

<p>$Y= X^T *\theta $
where
$\Theta=[{\theta^{(1)}},\theta^{(2)},…,\theta^{(n_u)}]$
$X=[{x^{(1)}},x^{(2)},…,x^{(n_m)}]$</p>

<h5 id="mathematics-about-low-rank-matrix-factorization">Mathematics about Low-Rank matrix factorization</h5>
<p>$\min |\boldsymbol A - \boldsymbol UV^{T} |_2 \text{,   subject to}~  rank(\boldsymbol UV^{T}) \leq r), where (|\cdot|_2$ denotes the Frobenius norm.</p>

<p>We start with the basic MF model, formulated as:</p>

\[\min _{\mathbf{U}, \mathbf{V}}\left\|\mathbf{X}-\mathbf{U} \mathbf{V}^{T}\right\|+\mathcal{L}(\mathbf{U}, \mathbf{V})\]

<p>where $X\in R^{m\times n}$ is the data matrix to be approximated, and $U\in R^{m\times k},V\in R^{n\times k}$ are two low-dimensional matrices ($k«min(m,m)$), $\mathcal{L}(U,V)$ is a regularization part to avoid overfitting.</p>

<h4 id="implementationvectorization-of-collaborative-filteringmean-normalization">Implementation/vectorization of Collaborative Filtering–Mean Normalization</h4>

<p>If we do not have a mean normalization process, then for a guy that gives no rating, the $\theta$ will be set to be 0. In this case, no recommendation can be made, as the predicted ratings will be 0 for any movie.</p>

<p><img src="/media/15718172032236/15720780158815.jpg" alt="-w815" width="500px" /></p>

<h4 id="implementation-of-mean-normalization">Implementation of mean normalization</h4>
<p>$Y=\left[\begin{array}{lllll}{5} &amp; {5} &amp; {0} &amp; {0} &amp; {?} \ {5} &amp; {?} &amp; {?} &amp; {0} &amp; {?} \ {?} &amp; {4} &amp; {0} &amp; {?} &amp; {?} \ {0} &amp; {0} &amp; {5} &amp; {4} &amp; {?} \ {0} &amp; {0} &amp; {5} &amp; {0} &amp; {?}\end{array}\right]$</p>

<p>then we can compute the average rating:
$\mu=\left[\begin{array}{c}{2.5} \ {2.5} \ {2} \ {2.25} \ {1.25}\end{array}\right]$</p>

<p>bsfun(@minus,Y,mu):</p>

\[Y=\left[\begin{array}{ccccc}{2.5} &amp; {2.5} &amp; {-2.5} &amp; {-2.5} &amp; {?} \\ {2.5} &amp; {?} &amp; {?} &amp; {-2.5} &amp; {?} \\ {?} &amp; {2} &amp; {-2} &amp; {?} &amp; {?} \\ {-2.25} &amp; {-2.25} &amp; {2.75} &amp; {1.75} &amp; {?} \\ {-1.25} &amp; {-1.25} &amp; {3.75} &amp; {-1.25} &amp; {?}\end{array}\right]\]

<p>Now for user j and movie i, the predicted rating is:
$&lt;\theta^{(j)},x^{(i)} &gt;+\mu_i$</p>

<p>In this case, even for the user who has not given any ratings, the ratings can be predicted as $0+\mu$.</p>
:ET