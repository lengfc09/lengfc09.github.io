I"ab<h2 id="test-statistic">Test Statistic</h2>

<p>A test is based on a statistic, which estimates the parameter that appears in the hypotheses
– Point estimate</p>

<p>Values of the estimate far from the parameter value in H0 give evidence against H0.</p>

<p>$H_a$ determines which direction will be counted as “far from the parameter value”.</p>

<p>Commonly, the test statistic has the form</p>

<p>$T=\frac{\text{estimate - hypothesized value}}{\text{standard deviation of the estimate} }$</p>

<h3 id="one-sample-t-test-test-statistic">One-Sample T Test: Test Statistic</h3>

<p>Parameter $\mu$ with hypothesized value $\mu_0$</p>

<p>Estimate $\bar{X}$ with observed value $\bar{x}$, and estimated standard deviation $s/\sqrt{n}$</p>

<p>Test statistics</p>

\[T=\frac{\bar{X}-\mu_0}{s/\sqrt{n}}\]

<p>with observed value</p>

\[t=\frac{\bar{x}-\mu_0}{s/\sqrt{n}}\]

<p><strong>State null and alternative hypothesis:</strong></p>

\[\begin{array}{r}
\mu \neq \mu_{0} \\
H_{0}: \mu=\mu_{0} \quad v s . \quad H_{a}: \quad \mu&gt;\mu_{0} \\
\mu&lt;\mu_{0}
\end{array}\]

<p>p-value equals, assuming $H_0$ holds</p>

\[2P(T\geq \vert t \vert )\\
P(T\geq t )\\
P(T\leq t )\]

<h3 id="hypothesis-testing-type-i-and-type-ii-errors">Hypothesis Testing: Type I and Type II Errors</h3>

<p><img src="/media/16010020114545/16010025027914.jpg" alt="-w849" width="800px" class="align-center" /></p>

<p>To limit the chance of a Type I Error to a chosen level α:</p>

<ul>
  <li>referred to as <em>significance level</em></li>
  <li>upper bound on Type I error</li>
  <li>commonly set at 5%</li>
</ul>

<p>Reject $H_0$ when the p-value &lt;= α</p>

<p>If so, we claim that the data support the alternative Ha at level α, or</p>

<p>– The data are statistically significant at level α</p>

<p>Relation between P-value and significance level α :</p>

<ul>
  <li>Reject H0 if p-value &lt;= α</li>
  <li>Do not reject H0 if p-value &gt; α.</li>
</ul>

<h3 id="two-sample-t-test-for-equal-means">Two-Sample t-Test for Equal Means</h3>

<h4 id="purpose-test-if-two-population-means-are-equal">Purpose: Test if two population means are equal</h4>

<p>The two-sample t-test (Snedecor and Cochran, 1989) is used to determine if two population means are equal. A common application is to test if a new process or treatment is superior to a current process or treatment.</p>

<p>There are several variations on this test.</p>

<ol>
  <li>
    <p>The data may either be paired or not paired. By paired, we mean that there is a one-to-one correspondence between the values in the two samples. That is, if X1, X2, …, Xn and Y1, Y2, … , Yn are the two samples, then Xi corresponds to Yi. For paired samples, the difference Xi - Yi is usually calculated. For unpaired samples, the sample sizes for the two samples may or may not be equal. The formulas for paired data are somewhat simpler than the formulas for unpaired data.</p>
  </li>
  <li>
    <p>The variances of the two samples may be assumed to be equal or unequal. Equal variances yields somewhat simpler formulas, although with computers this is no longer a significant issue.</p>
  </li>
  <li>
    <p>In some applications, you may want to adopt a new process or treatment only if it exceeds the current treatment by some threshold. In this case, we can state the null hypothesis in the form that the difference between the two populations means is equal to some constant μ1−μ2=d0 where the constant is the desired threshold.</p>
  </li>
</ol>

<h4 id="definition-of-two-sample-t-test">Definition of Two-Sample t-test</h4>

<p>The two-sample t-test for unpaired data is defined as:</p>

\[\begin{array}{ll}
\mathrm{H}_{0}: &amp; \mu_{1}=\mu_{2} \\
\mathrm{H}_{\mathrm{a}}: &amp; \mu_{1} \neq \mu_{2} \\
\text { Test Statistic: } &amp; T=\frac{\bar{Y}_{1}-\bar{Y}_{2}}{\sqrt{s_{1}^{2} / N_{1}+s_{2}^{2} / N_{2}}}
\end{array}\]

<p>where $N_1$ and $N_2$ are the sample sizes, $\bar{Y}_1$ and $\bar{Y}_2$ are the sample means, and $s_1^2$ and $s_2^2$ are the sample variances.</p>

<p>Significance Level: $\alpha$</p>

<p>Critical Region: Reject the null hypothesis that the two means are equal if</p>

\[\vert T \vert  &gt; t_{1-\alpha/2,v}\]

<p>where $t_{1-\alpha/2,v}$ is the critical value of t-distribution with v degrees of freedom.</p>

<p><strong>For the unequal variance case:</strong></p>

\[v=\frac{\left(s_{1}^{2} / N_{1}+s_{2}^{2} / N_{2}\right)^{2}}{\left(s_{1}^{2} / N_{1}\right)^{2} /\left(N_{1}-1\right)+\left(s_{2}^{2} / N_{2}\right)^{2} /\left(N_{2}-1\right)}\]

<p><strong>For the equal variance case:</strong></p>

\[v=N_1 + N_2 -2\]

<h4 id="two-sample-t-test-example">Two-Sample t-Test Example</h4>

<p>The following two-sample t-test was generated for the AUTO83B.DAT data set. The data set contains miles per gallon for U.S. cars (sample 1) and for Japanese cars (sample 2); the summary statistics for each sample are shown below.</p>

<p>SAMPLE 1</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre>    NUMBER OF OBSERVATIONS      = 249
    MEAN                        =  20.14458
    STANDARD DEVIATION          =   6.41470
    STANDARD ERROR OF THE MEAN  =   0.40652
</pre></td></tr></tbody></table></code></pre></div></div>

<p>SAMPLE 2</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre>    NUMBER OF OBSERVATIONS      = 79
    MEAN                        = 30.48101
    STANDARD DEVIATION          =  6.10771
    STANDARD ERROR OF THE MEAN  =  0.68717
</pre></td></tr></tbody></table></code></pre></div></div>

<p>We are testing the hypothesis that the population means are equal for the two samples. We assume that the variances for the two samples are equal.</p>

\[\mathrm{H}_{0}: \quad \mu_{1}=\mu_{2}\]

\[\mathrm{H}_{\mathrm{a}}: \quad \mu_{1} \neq \mu_{2}\]

<p>Test statistic: $T=-12.62059$</p>

<p>Pooled standard deviation: $\quad s_{p}=6.34260$</p>

<p>Degrees of freedom: $v=326$</p>

<p>significance level: $\alpha=0.05$</p>

<p>Critical value (upper tail): $t_{1-\alpha / 2, v}=1.9673$</p>

<p>Critical region: Reject $\mathrm{H}_{0}$ if $\vert T \vert &gt;1.9673$</p>

<p>The absolute value of the test statistic for our example, 12.62059, is greater than the critical value of 1.9673, so we reject the null hypothesis and conclude that the two population means are different at the 0.05 significance level.</p>

<p>In general, there are three possible alternative hypotheses and rejection regions for the one-sample t-test:</p>

\[\begin{array}{|l|l|}
\hline \text { Alternative Hypothesis } &amp; \text { Rejection Region } \\
\hline \mathrm{H}_{\mathrm{a}}: \mu_{1} \neq \mu_{2} &amp; |T|&gt;t_{1-\alpha / 2, v} \\
\hline \mathrm{H}_{\mathrm{a}}: \mu_{1}&gt;\mu_{2} &amp; T&gt;t_{1-\alpha, v} \\
\hline \mathrm{H}_{\mathrm{a}}: \mu_{1}&lt;\mu_{2} &amp; T&lt;t_{\alpha, v} \\
\hline
\end{array}\]

<p>For our two-tailed t-test, the critical value is $t_{1-\alpha / 2, v}  = 1.9673$, where α = 0.05 and ν = 326. If we were to perform an upper, one-tailed test, the critical value would be $t_{1-\alpha / 2, v} = 1.6495$. The rejection regions for three possible alternative hypotheses using our example data are shown below.</p>

<p><img src="/media/16010020114545/16011713863006.jpg" alt="-w700" width="700px" class="align-center" /></p>

<p><a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda353.htm">Engineering Statistics</a></p>

<h2 id="one-way-anova">One way ANOVA</h2>

<h3 id="one-way-anova-overview">One-way ANOVA overview</h3>

<p>In an analysis of variance, the variation in the response measurements is partitoined into components that correspond to different sources of variation.</p>

<p>The goal in this procedure is to split the total variation in the data into a portion due to random error and portions due to changes in the values of the independent variable(s).</p>

<p>The variance of n measurements is given by</p>

\[s^2 = \frac{\sum_{i=1}^n (y_i - \bar{y})^2}{n-1} \, ,\]

<p>where $\bar{y}$ is the mean of the n measurements.</p>

<p>The numerator part is called the <em>sum of squares of deviations from the mean</em>, and the denominator is called the <em>degrees of freedom</em>.</p>

<p>The SS in a one-way ANOVA can be split into two components, called the “sum of squares of treatments” and “sum of squares of error”, abbreviated as SST and SSE, respectively.</p>

<p>Algebraically, this is expressed by</p>

\[\begin{array}{ccccc}
SS(Total) &amp; = &amp; SST &amp; + &amp; SSE \\
          &amp;   &amp;     &amp;   &amp;     \\
\sum_{i=1}^k \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_{\huge{\cdot \cdot}})^2 &amp; = &amp;
\sum_{i=1}^k n_i (\bar{y}_{i \huge{\cdot}} - \bar{y}_{\huge{\cdot \cdot}})^2 &amp; + &amp;
\sum_{i=1}^k \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_{i \huge{\cdot}})^2 \, ,
\end{array}\]

<p>where k is the number of treatments and the bar over the $y_{\huge{\cdot \cdot}}$ denotes the “grand” or “overall” mean. Each $n_i$ is the number of observations for treatment i. The total number of observations is N (the sum of the $n_i$).</p>

<div class="definition">
We introduced the concept of treatment. The definition is: A treatment is a specific combination of factor levels whose effect is to be compared with other treatments.
</div>

<h3 id="the-one-way-anova-model-and-assumptions">The one-way ANOVA model and assumptions</h3>

<p>The mathematical model that describes the relationship between the response and treatment for the one-way ANOVA is given by</p>

\[Y_{ij} = \mu + \tau_i + \epsilon_{ij} \, ,\]

<p>where $Y_{ij}$ represents the j-th observation (j=1,2,…,$n_i$) on the i-th treatment (i=1,2,…,k levels). So, Y23 represents the third observation using level 2 of the factor. μ is the common effect for the whole experiment, τi represents the i-th treatment effect, and ϵij represents the random error present in the j-th observation on the i-th treatment.</p>

<p><strong>Fixed effects model</strong></p>

<p>The errors ϵij are assumed to be normally and independently (NID) distributed, with mean zero and variance $\sigma^2_{\varepsilon}$. $\mu$ is always a fixed parameter, and $\tau_1, \, \tau_2, \, \ldots, \, \tau_k$ are considered to be fixed parameters if the levels of the treatment are fixed and not a random sample from a population of possible levels. It is also assumed that $\mu$ is chosen so that</p>

\[\sum \tau_i = 0 \, , \,\,\,\,\, i = 1, \, \ldots, \, k\]

<p>holds. This is the fixed effects model.</p>

<p><strong>Random effects mode</strong></p>

<p>If the k levels of treatment are chosen at random, the model equation remains the same. However, now the $\tau_i$ values are random variables assumed to be $NID(0, \sigma_{\tau})$. This is the random effects model.</p>

<p>Whether the levels are fixed or random depends on how these levels are chosen in a given experiment.</p>

<h3 id="the-anova-table-and-tests-of-hypotheses-about-means">The ANOVA table and tests of hypotheses about means</h3>

<p><strong>Sums of Squares help us compute the variance estimates displayed in ANOVA Tables</strong>.</p>

<p>The sums of squares SST and SSE previously computed for the one-way ANOVA are used to form two mean squares, one for treatments and the second for error. These mean squares are denoted by MST and MSE, respectively. These are typically displayed in a tabular form, known as an ANOVA Table. The ANOVA table also shows the statistics used to test hypotheses about the population means.</p>

<p>When the null hypothesis of equal means is true, the two mean squares estimate the same quantity (error variance), and should be of approximately equal magnitude. In other words, their ratio should be close to 1. If the null hypothesis is false, MST should be larger than MSE.</p>

<p>Let $N=\sum n_i$. Then, the degrees of freedom for treatment are</p>

\[DFT = k - 1 \, ,\]

<p>and the degrees of freedom for error are</p>

\[DFE = N - k \, .\]

<p>The corresponding mean squares are:</p>

\[MST=SST/DFT\]

\[MSE=SSE/DFE\]

<h4 id="f-test">F-test</h4>

<p>The test statistic, used in testing the equality of treatment means is: F=MST/MSE.</p>

<p>The critical value is the tabular value of the F distribution, based on the chosen $\alpha$ level and the degrees of freedom DFT and DFE.</p>

<p>The calculations are displayed in an ANOVA table, as follows:</p>

\[\begin{array}{ccccc}
\hline \text { Source } &amp; \text { SS } &amp; \text { DF } &amp; \text { MS } &amp; \text { F } \\
\hline &amp; &amp; &amp; &amp; \\
\text { Treatments } &amp; S S T &amp; k-1 &amp; S S T /(k-1) &amp; M S T / M S E \\
\text { Error } &amp; S S E &amp; N-k &amp; S S E /(N-k) &amp; \\
\hline \text { Total (corrected) } &amp; S S &amp; N-1 &amp; &amp; \\
\hline
\end{array}\]

<p>The word “source” stands for source of variation. Some authors prefer to use “between” and “within” instead of “treatments” and “error”, respectively.</p>

<h4 id="anova-table-example">ANOVA Table Example</h4>

<p>The data below resulted from measuring the difference in resistance resulting from subjecting identical resistors to three different temperatures for a period of 24 hours. The sample size of each group was 5. In the language of design of experiments, we have an experiment in which each of three treatments was replicated 5 times.</p>

\[\begin{aligned}
&amp;\begin{array}{lccc}
\hline &amp;\text { Level 1 } &amp; \text { Level 2 } &amp; \text { Level 3 } \\
&amp;\hline 6.9 &amp; 8.3 &amp; 8.0 \\
&amp;5.4 &amp; 6.8 &amp; 10.5 \\
&amp;5.8 &amp; 7.8 &amp; 8.1 \\
&amp;4.6 &amp; 9.2 &amp; 6.9 \\
&amp;4.0 &amp; 6.5 &amp; 9.3 \\
\hline \text{mean} &amp;5.34 &amp; 7.72 &amp; 8.56 \\
\hline
\end{array}
\end{aligned}\]

<p>The resulting ANOVA table is</p>

\[\begin{array}{ccccc}
\hline \text { Source } &amp; \text { SS } &amp; \text { DF } &amp; \text { MS } &amp; \text { F } \\
\hline \text { Treatments } &amp; 27.897 &amp; 2 &amp; 13.949 &amp; 9.59 \\
\text { Error } &amp; 17.452 &amp; 12 &amp; 1.454 &amp; \\
\hline &amp; &amp; &amp; &amp; \\
\hline \text { Total (corrected) } &amp; 45.349 &amp; 14 &amp; &amp; \\
\text { Correction Factor } &amp; 779.041 &amp; 1 &amp; &amp;
\end{array}\]

<p>The test statistic is the F value of 9.59. Using an α of 0.05, we have $F_{0.05; \, 2, \, 12}= 3.89$.</p>

<p>Since the test statistic is much larger than the critical value, we reject the null hypothesis of equal population means and conclude that there is a (statistically) significant difference among the population means. The p-value for 9.59 is 0.00325, so the test statistic is significant at that level.</p>

<p>The populations here are resistor readings while operating under the three different temperatures. What we do not know at this point is whether the three means are all different or which of the three means is different from the other two, and by how much.</p>

<p>There are several techniques we might use to further analyze the differences. These are:</p>

<ul>
  <li><a href="https://www.itl.nist.gov/div898/handbook/prc/section4/prc435.htm">constructing confidence intervals around the difference of two means,</a></li>
  <li><a href="https://www.itl.nist.gov/div898/handbook/prc/section4/prc436.htm">estimating combinations of factor levels with confidence bounds</a></li>
  <li><a href="https://www.itl.nist.gov/div898/handbook/prc/section4/prc47.htm">multiple comparisons of combinations of factor levels tested simultaneously.</a></li>
</ul>

<h4 id="confidence-intervals-for-the-difference-of-treatment-means">Confidence intervals for the difference of treatment means</h4>

<p>This page shows how to construct a confidence interval around (μi−μj) for the one-way ANOVA by continuing the example shown on a previous page.</p>

<p>The formula for a 100(1−α) % confidence interval for the difference between two treatment means is:</p>

\[(\hat{\mu_i} - \hat{\mu_j}) \pm t_{1-\alpha/2, \, N-k} \,\,\sqrt{\hat{\sigma}^2_\epsilon \left(
\frac{1}{n_i}+\frac{1}{n_j}\right)} \, ,\]

<p>where $\hat{\sigma}_\epsilon^2 = MSE$.</p>

<p>For the example, we have the following quantities for the formula</p>

<ul>
  <li>$\bar{y}_3=8.56$</li>
  <li>$\bar{y}_1=5.34$</li>
  <li>$\sqrt{1.454(1/5 + 1/5)} = 0.763$</li>
  <li>$t_{0.975, \, 12} = 2.179$</li>
</ul>

<div class="info">
Here the degree of freedom is from $\hat{\sigma}_\epsilon^2$. As a result, in the t-statistic, the DOF is still the DOF of $\hat{\sigma}_\epsilon^2$, which is exactly N-k=15-3=12.

</div>

<p>Substituting these values yields (8.56 - 5.34) ± 2.179(0.763) or 3.22 ± 1.663.</p>

<p>That is, the confidence interval is (1.557, 4.883).</p>

<p>A 95 % confidence interval for μ3−μ2 is: (-1.787, 3.467).</p>

<p>A 95 % confidence interval for μ2−μ1 is: (-0.247, 5.007).</p>

<h3 id="application-employee-performance-study">Application: Employee Performance Study</h3>

<div class="exampl">
“Which of two prospective job candidates should we hire for a position that pays 80,000: the internal manager or the externally recruited manager?”
</div>

<p>Data set:</p>
<ul>
  <li>150 managers: 88 internal and 62 external</li>
  <li><strong>Manager Rating</strong> is an evaluation score of the employee in
their current job, indicating the “value” of the employee to the firm</li>
  <li><strong>Origin</strong> is a categorical variable that identifies the managers as either External or Internal to indicate from where they were hired</li>
  <li>Salary is the starting salary of the employee when they were hired. It indicates what sort of job the person was initially hired to do. In the context of this example, it does not measure how well they did that job. That’s measured by the rating variable.</li>
</ul>

<p><strong>Two Sample Comparison: Manager Rating vs Origin</strong></p>

<p><img src="/media/15951262104465/16012011732866.jpg" alt="-w800" width="800px" class="align-center" /></p>

<p>We can recognize a significant difference between the means via two-sample t-test.</p>

<p><strong>One-way ANOVA</strong></p>

<p><img src="/media/15951262104465/16012014511805.jpg" alt="-w600" width="600px" class="align-center" /></p>

<p><strong>Regress Manager Rating on Origin:</strong></p>

<p><img src="/media/16010020114545/16012016049817.jpg" alt="-w516" width="500px" class="align-center" /></p>

<ul>
  <li>The difference in the rating (-0.72) between internal and external managers is significant since the p-value = .003 &lt; .05.</li>
  <li>In terms of regression, Origin explains significant variation in Manager Rating.</li>
  <li>Before we claim that the external candidate should be hired, is there a possible confounding variable, another explanation for the difference in rating?</li>
  <li>Let’s explore the relationship between Manager Rating and Salary.</li>
</ul>

<p><strong>Scatterplot of Manager Rating vs. Salary</strong></p>

<p><img src="/media/16010020114545/16012016468506.jpg" alt="-w457" width="450px" class="align-center" /></p>

<ul>
  <li>(a) Salary is correlated with Manager Rating, and (b) that external managers were hired at higher salaries</li>
  <li>This combination indicates <strong>confounding</strong>: not only are we comparing internal vs. external managers; we are comparing internal managers hired into lower salary jobs with external managers placed into higher salary jobs.</li>
  <li><strong>Easy fix</strong>: compare only those whose starting salary near $80K. But that leaves too few data points for a reasonable comparison.</li>
</ul>

<p><strong>Separate Regressions of Manager Rating on Salary</strong></p>

<p><img src="/media/16010020114545/16012016859866.jpg" alt="-w872" width="800px" class="align-center" /></p>

<ul>
  <li>Based on the regression, at any given salary, internal managers is expected to get higher average ratings!</li>
  <li>In regression, confounding is a form of collinearity.
    <ul>
      <li>Salary is related to Origin which was the variable used to explain Rating.</li>
      <li>With Salary added, the effect of Origin changes sign. Now internal managers look better.</li>
    </ul>
  </li>
</ul>

<p><strong>Are the Two Fits Significantly Different?</strong></p>

<p><img src="/media/16010020114545/16012020493565.jpg" alt="-w512" width="500px" class="align-center" /></p>

<ul>
  <li>The two confidence bands overlap, which make the comparison indecisive.</li>
  <li>A more powerful idea is to combine these two separate simple regressions into one multiple regression that will allow us to compare these fits.</li>
</ul>

<p><strong>Regress Manager Rating on both Salary and Origin</strong></p>

<p><img src="/media/16010020114545/16012020885855.jpg" alt="-w826" width="800px" class="align-center" /></p>

<ul>
  <li>x1 dummy variable of being ’Internal’, I(Origin = Internal)</li>
  <li>Notice that we only require one dummy variable to distinguish internal from external managers.</li>
  <li>This enables two parallel lines for two kinds of managers.
    <ul>
      <li>If Origin = External, Manager Rating = -2.100459 + 0.107478 Salary</li>
      <li>If Origin = Internal, Manager Rating = -2.100459 + 0.107478 Salary + 0.514966</li>
    </ul>
  </li>
  <li>The coefficient of the dummy variable is the difference between the intercepts.</li>
  <li>The difference between the intercepts is significantly different from 0, since 0.0149, the p-value for Origin[Internal], is less than 0.05.</li>
  <li>Thus, if we assume the slopes are equal, a model using a categorical predictor implies that controlling for initial salary, internal managers rate significantly higher.</li>
  <li>How can we check the assumption that the slopes are parallel?</li>
</ul>

<p><strong>Model with Interaction: Different Slopes</strong></p>

<div class="definition">
Interaction. Beyond just looking at the plot, we can fit a model that allows the slopes to differ. This model gives an estimate of the difference between the slopes. This estimate is known as an interaction.
</div>

<p>An interaction between a dummy variable $I_k$ and a numerical variable $x_i$ measures the difference between the slopes of the numerical variable in the two groups:</p>

\[X_i * I_k\]

<p><img src="/media/16010020114545/16012023517103.jpg" alt="-w854" width="800px" class="align-center" /></p>

<p>Interaction variable – product of the dummy variable and Salary:</p>

\[\begin{aligned}
\text { originlnternal:salary } &amp;=\text { salary } &amp; &amp; \text { if Origin } &amp;=\text { Internal } \\
&amp;=0 &amp; &amp; \text { if Origin }=\text { External }
\end{aligned}\]

<ul>
  <li>If Origin = External:
    <ul>
      <li>Manager Rating = -1.94 + 0.11 Salary</li>
    </ul>
  </li>
  <li>If Origin = Internal:
    <ul>
      <li>Manager Rating = (-1.94+0.24) + (0.11+0.0037) Salary= -1.69 + 0.11 Salary</li>
    </ul>
  </li>
  <li>These equations match the simple regressions fit to the two groups separately. The interaction is not significant because its p-value is large.</li>
</ul>

<p><strong>Principle of Marginality</strong></p>

<ul>
  <li>Leave main effects in the model (here Salary and Origin) whenever an interaction that uses them is present in the fitted model. If the interaction is not statistically significant, remove the interaction from the model.</li>
  <li>Origin became insignificant when Salary∗Origin was added, which is due to collinearity.</li>
  <li>The assumption of equal error variance should also be checked by comparing box-plots of the residuals grouped by the levels of the categorical variable.</li>
</ul>

<p><img src="/media/16010020114545/16012025346607.jpg" alt="-w378" width="400px" class="align-center" /></p>

<p><strong>Summary of this example</strong></p>

<ul>
  <li>Categorical variables model the differences between groups using regression, while taking account of other variables.</li>
  <li>In a model with a categorical variable, the coefficients of the categorical terms indicate <strong>differences between parallel lines</strong>.</li>
  <li>In a model that includes interactions, the coefficients of the interaction measure the <strong>differences in the slopes between the groups.</strong></li>
  <li>Significant categorical variable ⇒ different intercepts</li>
  <li>Significant interaction ⇒ different slopes</li>
</ul>

<h2 id="statistical-significance-and-practical-significance">Statistical Significance and Practical Significance</h2>

<p>When drawing conclusions from a hypothesis test, it is important to keep in mind the difference between Statistical and Practical Significance.</p>

<ul>
  <li>Statistical Significance : We can be sure that !” is false i.e. the difference from the hypothesized value is too large to be attributed to chance. Statistics can answer this question.</li>
  <li>Practical Significance : Is the difference large enough that in practice we care? Statistics can not answer this one!</li>
</ul>
:ET