I"§><p class="notice--info">All of this series is mainly based on the Machine Learning course given by Andrew Ng, which is hosted on <a href="https://www.coursera.org/">cousera.org</a>.</p>

<h2 id="unsupervised-learning--clustering">Unsupervised Learning â€“ Clustering</h2>
<h3 id="k-means-algorithms">K-means Algorithms</h3>

\[\begin{array}{l}{\text { K-means algorithm }} \\ {\text { Input: }} \\ {\qquad \begin{array}{l}{-\quad K( \text { number of clusters) } } \\ {-\quad \text { Training set }\left\{x^{(1)}, x^{(2)}, \ldots, x^{(m)}\right\}} \\ {x^{(i)} \in \mathbb{R}^{n}\left(\text { drop } x_{0}=1 \text { convention }\right)}\end{array}}\end{array}\]

<p>Algorithm:</p>

\[\begin{array}{l}{\text { K-means algorithm }} \\ {\text { Randomly initialize } K \text { cluster centroids } \mu_{1}, \mu_{2}, \ldots, \mu_{K} \in \mathbb{R}^{n}} \\ {\text { Repeat }\{} \\ {\qquad \begin{aligned}\left.c^{(i)} :=\text { index ( from } 1 \text { to } K\right) \text { of cluster centroid }  \text { closest to } x^{(i)} \\ \mu_{k} :=\text { average (mean) of points assigned to cluster } k \end{aligned}}  \\ {}\end{array}\]

<p>K-means for Non-separated clusters</p>

<p><img src="/media/15718172032236/15718196173619.jpg" alt="-w652" width="652px" /></p>

<h3 id="optimization-objective-for-k-means">Optimization Objective for K-means</h3>

\[\begin{array}{l}{J\left(c^{(1)}, \ldots, c^{(m)}, \mu_{1}, \ldots, \mu_{K}\right)=\frac{1}{m} \sum_{i=1}^{m}\left\|x^{(i)}-\mu_{c^{(i)}}\right\|^{2}} \\ {\min _{c^{(1)}, \ldots, c^{(m)}} J\left(c^{(1)}, \ldots, c^{(m)}, \mu_{1}, \ldots, \mu_{K}\right)} \\ {\mu_{1}, \ldots, \mu_{K}}\end{array}\]

<p>where
\(\begin{aligned} c^{(i)} &amp;=\text { index of cluster }(1,2, \ldots, K) \text { to which example } x^{(i)} \text { is currently } \\ &amp; \text { assigned } \\ \mu_{k} &amp;=\text { cluster centroid } k\left(\mu_{k} \in \mathbb{R}^{n}\right) \end{aligned}\)</p>

\[\begin{aligned} \mu_{c^{(i)}} &amp;=\text { cluster centroid of cluster to which example } x^{(i)} \text { has been } \\ &amp; \text { assigned } \end{aligned}\]

<p><img src="/media/15718172032236/15718355986198.jpg" alt="-w500" width="500px" /></p>

<p>It is equivalent to repeatedly solving the optimization problem for $C^{(i)} ï½žfor ~i=1,â€¦,m$ first, then for $\mu_k ~for ~k=1,â€¦,K$</p>

<h3 id="initial-guesses---random-initialization">Initial Guesses - Random Initialization</h3>

<p>Bad starting point can lead to local optimum:</p>

<p><img src="/media/15718172032236/15718367117880.jpg" alt="-w895" width="500px" /></p>

<p>Solution: more trials with different starting point.
Then pick the lowers cost function.</p>

<h3 id="choose-the-number-of-clusters">Choose the number of clusters</h3>

<p><img src="/media/15718172032236/15718385140287.jpg" alt="-w510" width="510px" /></p>

<p>Evaluate it based on the later/downstream purposes.</p>

<p><img src="/media/15718172032236/15718386040729.jpg" alt="-w1039" width="500px" /></p>

<h2 id="dimensionality-reduction">Dimensionality Reduction</h2>
<h3 id="motivation">Motivation</h3>

<ul>
  <li>Data</li>
  <li>Data Visualization</li>
</ul>

<p><img src="/media/15718172032236/15718450287423.jpg" alt="-w916" width="500px" /></p>

<p><img src="/media/15718172032236/15718453210197.jpg" alt="-w542" width="500px" /></p>

<p>But what is the meaning of these new features? -&gt; it is a difficult to explain.</p>
<h3 id="principle-component-analysis">Principle Component Analysis</h3>

<p><img src="/media/15718172032236/15718459312632.jpg" alt="-w572" width="572px" /></p>

\[\begin{array}{l}{\text { Reduce from n-dimension to k-dimension: Find } k \text { vectors } u^{(1)}, u^{(2)}, \ldots, u^{(k)}} \\ {\text { onto which to project the data, so as to minimize the projection error. }}\end{array}\]

<p><img src="/media/15718172032236/15718470092968.jpg" alt="-w800" width="800px" /></p>

<p>For the linear regression:
$min ||Ax-y||^2$
since $Ax=(a1,a2,â€¦,a_n)x$, the linear regression is essentially the projection of y onto the linear space of column vectors of A.</p>

<p>For the PCA, the first principle component is the one that:
$\max ||A\mu||^2 $</p>

<p>Mathematically, for the linear regression, it is a problem of project a point in $R^m $ into the linear space of ${x_0,x_1,â€¦,x_n}$. In other words, the base is given.</p>

<p>for the PCA, however, it is a problem of find a sub-space for the data given, so that most of the information can be maintained. In other words, the task is to find the base of the sub-space. ($\mu_1, â€¦, \mu_k \in R^n$)</p>

<h4 id="details-in-mathematics">Details in Mathematics</h4>

<p><strong>First Component</strong></p>

<p>In order to maximize variance, the first weight vector w(1) thus has to satisfy:</p>

\[\begin{equation}
\mathbf{w}_{(1)}=\underset{\|\mathbf{w}\|=1}{\arg \max }\left\{\sum_{i}\left(t_{1}\right)_{(i)}^{2}\right\}=\underset{\|\mathbf{w}\|=1}{\arg \max }\left\{\sum_{i}\left(\mathbf{x}_{(i)} \cdot \mathbf{w}\right)^{2}\right\}
\end{equation}\]

<p>Equivalently, writing this in matrix form gives</p>

\[{\displaystyle \mathbf {w} _{(1)}={\underset {\Vert \mathbf {w} \Vert =1}{\operatorname {\arg \,max} }}\,\{\Vert \mathbf {Xw} \Vert ^{2}\}={\underset {\Vert \mathbf {w} \Vert =1}{\operatorname {\arg \,max} }}\,\left\{\mathbf {w} ^{T}\mathbf {X^{T}} \mathbf {Xw} \right\}}\]

<p>Since $w_{(1)}$ has been defined to be a unit vector, it equivalently also satisfies</p>

\[{\displaystyle \mathbf {w} _{(1)}={\underset {\Vert \mathbf {w} \Vert =1}{\operatorname {\arg \,max} }}\,\{\Vert \mathbf {Xw} \Vert ^{2}\}={\underset {\Vert \mathbf {w} \Vert =1}{\operatorname {\arg \,max} }}\,\left\{\mathbf {w} ^{T}\mathbf {X^{T}} \mathbf {Xw} \right\}}\]

<p>The quantity to be maximised can be recognised as a Rayleigh quotient. A standard result for a positive semidefinite matrix such as $X^TX$  is that the quotientâ€™s maximum possible value is the largest <strong>eigenvalue</strong> of the matrix, which occurs when w is the corresponding <strong>eigenvector</strong>.</p>

<p>With $w_{(1)}$ found, the first principal component of a data vector $x_{(i)}$ can then be given as a score $t_{1(i)} = x_{(i)} â‹… w_{(1)}$ in the transformed co-ordinates,  or as the corresponding vector in the original variables $t_{1(i)}w_{(1)} = &lt;x_{(i)} * w_{(1)}&gt;w_{(1)}$</p>

<p><strong>Further Component</strong></p>

<p>The kth component can be found by subtracting the first k âˆ’ 1 principal components from X:</p>

\[\begin{equation}
\hat{\mathbf{X}}_{k}=\mathbf{X}-\sum_{s=1}^{k-1} \mathbf{X} \mathbf{w}_{(s)} \mathbf{w}_{(s)}^{\mathrm{T}}
\end{equation}\]

<p>and then finding the weight vector which extracts the maximum variance from this new data matrix</p>

\[\begin{equation}
\mathbf{w}_{(k)}=\underset{\|\mathbf{w}\|=1}{\arg \max }\left\{\left\|\hat{\mathbf{X}}_{k} \mathbf{w}\right\|^{2}\right\}=\arg \max \left\{\frac{\mathbf{w}^{T} \hat{\mathbf{X}}_{k}^{T} \hat{\mathbf{X}}_{k} \mathbf{w}}{\mathbf{w}^{T} \mathbf{w}}\right\}
\end{equation}\]

<p>It turns out that this gives the remaining eigenvectors of $X^TX$, with the maximum values for the quantity in brackets given by their corresponding eigenvalues. Thus the weight vectors are eigenvectors of $X^TX$.
The full principal components decomposition of X can therefore be given as
$\mathbf{T} = \mathbf{X} \mathbf{W}$
where W is a p-by-p matrix of weights whose columns are the eigenvectors of $X^TX$.  The transpose of W is sometimes called the whitening or sphering transformation.
Columns of W multiplied by the square root of corresponding eigenvalues, i.e. eigenvectors scaled up by the variances, are called loadings in PCA or in Factor analysis.</p>

<h4 id="pca-algorithm">PCA algorithm</h4>
<p><strong>Step 1: Data Preprocessing</strong>
Mean Normalization, so that the features centers in the original point.
Feature scaling is necessary;</p>

\[X=\frac{[X-mean(X)]}{std(X)}\]

<p><img src="/media/15718172032236/15718483847487.jpg" alt="-w880" width="500px" /></p>

<p><img src="/media/15718172032236/15718487654558.jpg" alt="-w507" width="500px" /></p>

<h4 id="vectorization">Vectorization:</h4>
<p>Mean normalization and optionally feature scaling:</p>

\[X= \text{bsxfun}(@minus, X, mean(X,1))\]

\[\sum =\frac{1}{m} X^TX\]

\[[U,S,V]=svd(\sum )\]

<p>Then we have:</p>

\[U=\left[\begin{array}{cccc}{|} &amp; {|} &amp; {} &amp; {|} \\ {u^{(1)}} &amp; {u^{(2)}} &amp; {\ldots} &amp; {u^{(n)}} \\ {|} &amp; {|} &amp; {} &amp; {|}\end{array}\right] \in \mathbb{R}^{n \times n}\]

<p>$x\in R^n \to z\in R^k: $</p>

<p>$\text{Ureduce}=U(~: ~, 1:k)$</p>

<p>$z=\text{Ureduce}^T*X$</p>

<p>Note 1: $x_0^{i} \neq 0$ for this convention.
Note 2: $U$ is from $USV^*=X^TX$, therefore U is $R^{n\times n}$. It is the eigenvector of X.</p>
<h3 id="applying-pca">Applying PCA</h3>
<h4 id="reconstruction-from-compressed-representation">Reconstruction from compressed representation</h4>

\[z=\text{Ureduce}^T*X \Longrightarrow X_{Approx}=\text{Ureduce}*z\]

<p>here $ X_{Approx } \in R^n $</p>

<h4 id="choose-the-number-of-principle-component">Choose the number of principle component</h4>

<p><img src="/media/15718172032236/15719080855261.jpg" alt="-w594" width="594px" /></p>

<p>Since S is the eigenvalues for $X^TX$, $s_ii$ is actually the square of $s_i$, which is the eigenvalue for X.</p>

\[\begin{array}{l}{\text { Choosing } k \text { (number of principal components) }} \\ {[\mathrm{U}, \mathrm{S}, \mathrm{V}]=\mathrm{svd}(\text { Sigma })} \\ {\text { Pick smallest value of } k \text { for which }} \\ {\qquad \frac{\sum_{i=1}^{k} S_{i i}}{\sum_{i=1}^{m} S_{i i}} \geq 0.99} \\ {\text { (99% of variance retained) }}\end{array}\]

<h3 id="advice-for-applying-pca">Advice for applying PCA</h3>
<h4 id="supervised-learning-speedup">Supervised learning speedup</h4>

<p><img src="/media/15718172032236/15719091055242.jpg" alt="-w617" width="617px" /></p>

<p>Note: must normalize the X before PCA. feature scaling is optional but recommended if large variance among features.</p>

<p>The same transformation must be done for $x_{val}$ and $x_{test}$</p>

<p>In summary, application for PCA:</p>
<ul>
  <li>Compression
    <ul>
      <li>Reduce memory/disk needed for storage of data</li>
      <li>speed up learning algorithm</li>
    </ul>
  </li>
  <li>Visualization</li>
</ul>

<p>Note: It might work ok, but it is generally a bad application to use PCA to prevent overfitting. Use regularization instead</p>

\[\min _{\theta} \frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}\]

<p>If the overfitting is the problem, utilization of PCA would probably throw away some useful information.</p>

<p>People like to utilized PCA for any ML problem:</p>

\[\begin{array}{l}{\text { Design of ML system: }} \\ {\text { - Get training set }\left\{\left(x^{(1)}, y^{(1)}\right),\left(x^{(2)}, y^{(2)}\right), \ldots,\left(x^{(m)}, y^{(m)}\right)\right\}} \\ {\left.\left. \text { - Run PCA to reduce } x^{(i)} \text { in dimension to get } z^{(i)} \right)\right\}} \\ {\text { - Train logistic regression on }\left\{\left(z^{(1)}, y^{(1)}\right), \ldots,\left(z^{(m)}, y^{(m)}\right)\right\}} \\ {\text { - Test on test set: Map } x_{t e s t}^{(i)} \text { to } z_{test}^{(i)} . \text { Run } h_{\theta}(z) \text { on }} \\ {\quad\left\{\left(z_{t e s t}^{(1)}, y_{t e s t}^{(1)}\right), \ldots,\left(z_{t e s t}^{(m)}, y_{t e s t}^{(m)}\right)\right\}}\end{array}\]

<p>Before implementing PCA, it is better to try running without the PCA with the original/raw data. Only if it does not do what is desired, then implement PCA.</p>

<h2 id="anomaly-detection">Anomaly Detection</h2>
<h3 id="density-estimation">Density Estimation</h3>
<h4 id="problem-motivation">Problem Motivation</h4>
<p><img src="/media/15718172032236/15719110811698.jpg" alt="-w497" width="497px" /></p>

<p><img src="/media/15718172032236/15719111371947.jpg" alt="-w474" width="474px" /></p>

<h4 id="algorithm-for-a-anomaly-detection">Algorithm for a Anomaly detection</h4>

<p><img src="/media/15718172032236/15719183544358.jpg" alt="-w529" width="529px" /></p>

<h3 id="building-anomaly-detection-system">Building Anomaly Detection System</h3>

<p><img src="/media/15718172032236/15719198195844.jpg" alt="-w513" width="513px" /></p>

<p>Due to possible skewness of the data or label, the accuracy of prediction is not a good measure of the performance of algorithm.</p>

<ul>
  <li>Possible evaluation metrics:</li>
</ul>

<p><strong>Precision</strong>:</p>

\[\frac{\# True ~Positives}{\#~ Total~Predicted~Positives}\]

<p><strong>Recall</strong>:</p>

\[\frac{\# True ~Positives}{\#~ Total~Actual~Positives}\]

<p><img src="/media/15689769464157/15689791419155.jpg" alt="-w549" width="549px" /></p>

<p>$F_1 ~ Score=2 \frac{PR}{P+R}$</p>

<ul>
  <li>Use cross validation set to choose parameter $\epsilon  $</li>
</ul>

<h4 id="anomaly-detection-gaussian-vs-supervised-learning">Anomaly Detection (Gaussian) VS. Supervised Learning</h4>
<p>Since we have labeled data, why not just use supervised learning to detect to anomaly?</p>

<p><img src="/media/15718172032236/15719205588140.jpg" alt="-w592" width="592px" /></p>

<p>Few features: use anomaly detection, as the limited number of features make it hard to learn.</p>

<p><img src="/media/15718172032236/15719207332155.jpg" alt="-w595" width="595px" /></p>

<h4 id="choosing-what-features-to-use-in-the-anomaly-detection">Choosing What Features to Use in the Anomaly Detection</h4>
<ul>
  <li>If the feature is Non-Gaussian, it is better to transform the feature to be more like Gaussian distribution</li>
  <li>If it is difficult to identify the anomaly with the current features, it will helpful if more features can be added, specially based on observation of anomaly.
    <h3 id="multivariant-gaussian-distribution">Multivariant Gaussian Distribution</h3>
    <p>In the previous anomaly detection algorithm, the features are assumed to be independent and normally distributed. If we want to consider the dependency or correlation among the n features, we can use multivariant gaussian distribution.</p>
  </li>
</ul>

<p><img src="/media/15718172032236/15719224082729.jpg" alt="-w908" width="908px" /></p>

<p>Note: use PCA to capture the normal?</p>

\[\begin{array}{l}{\text { Multivariate Gaussian (Normal) distribution }} \\ {x \in \mathbb{R}^{n} . \text { Don't model } p\left(x_{1}\right), p\left(x_{2}\right), \ldots, \text { etc. separately. }} \\ {\text { Model } p(x) \text { all in one go. }} \\ {\text { Parameters: } \mu \in \mathbb{R}^{n}, \Sigma \in \mathbb{R}^{n \times n} \text { (covariance matrix) }}\end{array}\]

\[{\displaystyle f_{\mathbf {X} }(x_{1},\ldots ,x_{k})={\frac {\exp \left(-{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm {T} }{\boldsymbol {\Sigma }}^{-1}({\mathbf {x} }-{\boldsymbol {\mu }})\right)}{\sqrt {(2\pi )^{k}|{\boldsymbol {\Sigma }}|}}}}\]

\[\begin{array}{l}{\text { Parameters } \mu, \Sigma} \\ {\qquad p(x ; \mu, \Sigma)=\frac{1}{(2 \pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right)}\end{array}\]

<ul>
  <li>Parameter Filtering:
Given training set ${x^{(1)}, x^{(2)}, \ldots, x^{(m)} }$, we can calculate the parameters:</li>
</ul>

\[\mu=\frac{1}{m} \sum_{i=1}^{m} x^{(i)} \quad \Sigma=\frac{1}{m} \sum_{i=1}^{m}\left(x^{(i)}-\mu\right)\left(x^{(i)}-\mu\right)^{T}\]

<p>here $\mu , x^{(i)} \in R^n$, in other words:
$\sum =E(X-\mu)^T (X-\mu)$
here $X=[{x^{(1)}}â€™;{x^{(2)}}â€™;â€¦;{x^{(m)}}â€™]$
therefore we have:
$Xâ€™=[{x^{(1)}},{x^{(2)}},â€¦,{x^{(m)}}]$</p>

<ul>
  <li>Algorithm:</li>
</ul>

<p><img src="/media/15718172032236/15719333498155.jpg" alt="-w548" width="548px" /></p>

<p>Note that for the multivariate gaussion:</p>
<ul>
  <li>the n features $\in R^m$ must be linearly independent, i.e. full rank of n:==&gt; must have m$\geq$n, otherwise we have some $x^T \sum x=0$ which means $\sum $ is non-invertible.</li>
  <li>Computationally more expensive</li>
</ul>
:ET